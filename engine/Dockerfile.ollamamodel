# # Base: Use Ollama official base (Debian/Ubuntu + Ollama runtime)
# FROM ollama/ollama:latest

# # Copy your application into container
# # WORKDIR /app
# # COPY . /app

# # Ensure Ollama model is preloaded (can also mount via PVC if >10GB models)
# RUN ollama serve & sleep 5 && ollama pull llama3.1:8b

# # Expose Ollama API port
# EXPOSE 11434

# # Start Ollama server and your app together (example: python app.py)
# # CMD ["sh", "-c", "ollama serve & sleep 5 && python /app/app.py"]
# CMD ["ollama ","serve", "&"," sleep", "5"]

# Use the official Ollama base image
lama:latest

# This RUN command is the key. It starts the server in the background,
# waits a moment for it to be ready, pulls the desired model,
# and then the server process exits, leaving the model in the image layer.
RUN /bin/sh -c 'ollama serve & sleep 5 && ollama pull llama3.1:8b'

# The default entrypoint of the base image is to run "ollama serve",
# so we don't need to specify CMD or ENTRYPOINT again.

# FROM ollama/ollama:latest

# Copy the pre-downloaded models from the build context
# into the expected directory inside the container.
# COPY ./local_models /root/.ollama/models

